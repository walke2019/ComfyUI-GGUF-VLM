"""
Transformers Inference Engine - åŸºäº Transformers çš„æ¨ç†å¼•æ“
æ”¯æŒ Qwen3-VL ç­‰ Transformers æ¨¡å‹ï¼ˆä½¿ç”¨æœ€æ–° APIï¼‰
"""

import os
import sys
import torch
import shutil
from typing import Dict, Optional, List, Any
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
module_path = Path(__file__).parent.parent.parent
if str(module_path) not in sys.path:
    sys.path.insert(0, str(module_path))

# å¯¼å…¥é…ç½®å’Œå·¥å…·ï¼ˆComfyUI ç¯å¢ƒå…¼å®¹ï¼‰
try:
    # æ–¹å¼1: å°è¯•ä»å½“å‰åŒ…å¯¼å…¥
    from config.paths import PathConfig
    from utils.download_manager import get_download_manager
except ImportError:
    try:
        # æ–¹å¼2: å°è¯•ç›¸å¯¹å¯¼å…¥
        from ...config.paths import PathConfig
        from ...utils.download_manager import get_download_manager
    except (ImportError, ValueError):
        # æ–¹å¼3: åŠ¨æ€å¯¼å…¥ï¼ˆæœ€å¯é ï¼‰
        import importlib.util
        
        # å¯¼å…¥ PathConfig
        paths_file = module_path / 'config' / 'paths.py'
        spec = importlib.util.spec_from_file_location('config.paths', paths_file)
        paths_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(paths_module)
        PathConfig = paths_module.PathConfig
        
        # å¯¼å…¥ get_download_manager
        dm_file = module_path / 'utils' / 'download_manager.py'
        spec = importlib.util.spec_from_file_location('utils.download_manager', dm_file)
        dm_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(dm_module)
        get_download_manager = dm_module.get_download_manager


class TransformersInferenceEngine:
    """Transformers æ¨ç†å¼•æ“ï¼ˆQwen3-VL ä¼˜åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.current_model_id = None
        self.current_config = None
        
    def load_model(self, config: Dict) -> bool:
        """
        åŠ è½½æ¨¡å‹
        
        Args:
            config: æ¨¡å‹é…ç½®
                - model_name: æ¨¡å‹åç§°
                - model_id: HuggingFace æ¨¡å‹ ID
                - quantization: é‡åŒ–ç±»å‹ (none/4bit/8bit)
                - attention: æ³¨æ„åŠ›æœºåˆ¶ (eager/sdpa/flash_attention_2)
                - device: è®¾å¤‡
                - dtype: æ•°æ®ç±»å‹
                - min_pixels: æœ€å°åƒç´ 
                - max_pixels: æœ€å¤§åƒç´ 
        
        Returns:
            æ˜¯å¦æˆåŠŸåŠ è½½
        """
        try:
            from transformers import (
                AutoModelForVision2Seq,
                AutoProcessor,
                BitsAndBytesConfig,
            )
            
            # å°è¯•å¯¼å…¥ Qwen3VL ç‰¹å®šç±»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                from transformers import Qwen3VLForConditionalGeneration
                use_qwen3vl = True
            except ImportError:
                use_qwen3vl = False
                print("âš ï¸  Qwen3VLForConditionalGeneration not available, using AutoModelForVision2Seq")
            import comfy.model_management
            
            model_id = config.get('model_id')
            model_name = config.get('model_name', model_id)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½
            if (self.current_model_id == model_id and 
                self.current_config == config and
                self.model is not None and 
                self.processor is not None):
                print(f"âœ… Model already loaded: {model_name}")
                return True
            
            # æ¸…ç†æ—§æ¨¡å‹
            if self.model is not None or self.processor is not None:
                self._unload_model()
            
            # ä½¿ç”¨ç»Ÿä¸€çš„è·¯å¾„é…ç½®
            model_checkpoint = PathConfig.get_model_path("llm", model_id)
            
            # ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
            # æ£€æŸ¥å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            key_files = [
                "config.json",
                "model.safetensors.index.json",
            ]
            
            needs_download = not os.path.exists(model_checkpoint)
            if os.path.exists(model_checkpoint):
                # æ£€æŸ¥æ˜¯å¦æœ‰å…³é”®æ–‡ä»¶ç¼ºå¤±
                for key_file in key_files:
                    if not os.path.exists(os.path.join(model_checkpoint, key_file)):
                        needs_download = True
                        print(f"âš ï¸ Missing key file: {key_file}, will re-download")
                        break
            
            if needs_download:
                download_manager = get_download_manager()
                
                # æ£€æŸ¥ç£ç›˜ç©ºé—´
                if not download_manager.check_disk_space(model_checkpoint, required_gb=10.0):
                    raise RuntimeError("Insufficient disk space for model download")
                
                # ä¸‹è½½æ¨¡å‹
                success = download_manager.download_repository(
                    repo_id=model_id,
                    local_dir=model_checkpoint,
                    ignore_patterns=[
                        "*.gguf",
                        "GGUF/*",
                        "*.bin",
                        "*.msgpack",
                    ],
                    resume=True
                )
                
                if not success:
                    raise RuntimeError(f"Failed to download model: {model_id}")
            
            # åŠ è½½ Processorï¼ˆQwen3-VL ä¸éœ€è¦ min_pixels/max_pixels å‚æ•°ï¼‰
            print(f"ğŸ“¦ Loading processor from: {model_checkpoint}")
            self.processor = AutoProcessor.from_pretrained(model_checkpoint)
            
            # é…ç½®é‡åŒ–
            quantization = config.get('quantization', 'none')
            quantization_config = None
            
            if quantization == '4bit':
                quantization_config = BitsAndBytesConfig(load_in_4bit=True)
                print("ğŸ”§ Using 4-bit quantization")
            elif quantization == '8bit':
                quantization_config = BitsAndBytesConfig(load_in_8bit=True)
                print("ğŸ”§ Using 8-bit quantization")
            
            # ç¡®å®šæ•°æ®ç±»å‹
            device = comfy.model_management.get_torch_device()
            bf16_support = (
                torch.cuda.is_available() and
                torch.cuda.get_device_capability(device)[0] >= 8
            )
            dtype = torch.bfloat16 if bf16_support else torch.float16
            
            # åŠ è½½æ¨¡å‹
            print(f"ğŸ“¦ Loading model: {model_name}")
            attention = config.get('attention', 'sdpa')
            
            # Qwen3-VL æ¨èä½¿ç”¨ flash_attention_2
            if attention == 'flash_attention_2':
                print("âš¡ Using Flash Attention 2 (recommended for Qwen3-VL)")
            
            model_kwargs = {
                "dtype": dtype,
                "device_map": "auto",
            }
            
            # åªåœ¨éé‡åŒ–æ—¶æ·»åŠ  attn_implementation
            if quantization == 'none':
                model_kwargs["attn_implementation"] = attention
            
            if quantization_config:
                model_kwargs["quantization_config"] = quantization_config
            
            # æ ¹æ®å¯ç”¨æ€§é€‰æ‹©æ¨¡å‹ç±»
            ModelClass = Qwen3VLForConditionalGeneration if use_qwen3vl else AutoModelForVision2Seq
            
            self.model = ModelClass.from_pretrained(
                model_checkpoint,
                **model_kwargs
            )
            
            self.current_model_id = model_id
            self.current_config = config.copy()
            
            print(f"âœ… Model loaded successfully: {model_name}")
            print(f"   Location: {model_checkpoint}")
            print(f"   Device: {device}")
            print(f"   Dtype: {dtype}")
            print(f"   Attention: {attention}")
            print(f"   Quantization: {quantization}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def inference(
        self,
        messages: List[Dict],
        temperature: float = 0.7,
        max_new_tokens: int = 2048,
        seed: int = 0,
        top_p: float = 0.8,
        top_k: int = 20,
        repetition_penalty: float = 1.0
    ) -> str:
        """
        æ‰§è¡Œæ¨ç†ï¼ˆä½¿ç”¨ Qwen3-VL æ–° APIï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            seed: éšæœºç§å­
            top_p: nucleus sampling å‚æ•°
            top_k: top-k sampling å‚æ•°
            repetition_penalty: é‡å¤æƒ©ç½š
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if self.model is None or self.processor is None:
            raise RuntimeError("Model not loaded. Call load_model() first.")
        
        try:
            # è®¾ç½®éšæœºç§å­
            if seed > 0:
                torch.manual_seed(seed)
                if torch.cuda.is_available():
                    torch.cuda.manual_seed_all(seed)
            
            with torch.no_grad():
                # ä½¿ç”¨ 1038lab/ComfyUI-QwenVL çš„æ–¹å¼ï¼šå…ˆç”Ÿæˆæ–‡æœ¬æç¤ºï¼Œå†å¤„ç†å›¾åƒ
                # Step 1: ç”Ÿæˆæ–‡æœ¬æç¤ºï¼ˆä¸ tokenizeï¼‰
                text_prompt = self.processor.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                
                # Step 2: æå–å›¾åƒ
                pil_images = []
                for msg in messages:
                    if isinstance(msg.get('content'), list):
                        for item in msg['content']:
                            if item.get('type') == 'image':
                                # å›¾åƒå·²ç»æ˜¯ PIL Image æˆ–è·¯å¾„
                                from PIL import Image
                                img = item.get('image')
                                if isinstance(img, str):
                                    pil_images.append(Image.open(img))
                                elif isinstance(img, Image.Image):
                                    pil_images.append(img)
                
                # Step 3: ä½¿ç”¨ processor å¤„ç†æ–‡æœ¬å’Œå›¾åƒ
                inputs = self.processor(
                    text=text_prompt,
                    images=pil_images if pil_images else None,
                    return_tensors="pt"
                )
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                model_inputs = {k: v.to(self.model.device) for k, v in inputs.items() if torch.is_tensor(v)}
                
                # ç”Ÿæˆå‚æ•°ï¼ˆéµå¾ª 1038lab/ComfyUI-QwenVL çš„æ–¹å¼ï¼‰
                stop_tokens = [self.processor.tokenizer.eos_token_id]
                if hasattr(self.processor.tokenizer, 'eot_id'):
                    stop_tokens.append(self.processor.tokenizer.eot_id)
                
                generation_config = {
                    "max_new_tokens": max_new_tokens,
                    "repetition_penalty": repetition_penalty,
                    "eos_token_id": stop_tokens,
                    "pad_token_id": self.processor.tokenizer.pad_token_id,
                    "do_sample": temperature > 0,
                    "temperature": temperature if temperature > 0 else None,
                    "top_p": top_p if temperature > 0 else None,
                    "top_k": top_k if temperature > 0 else None,
                }
                
                # ç§»é™¤ None å€¼
                generation_config = {k: v for k, v in generation_config.items() if v is not None}
                
                print(f"ğŸ” Inference config: temp={temperature}, max_tokens={max_new_tokens}, top_p={top_p}, rep_penalty={repetition_penalty}")
                
                # ç”Ÿæˆ
                generated_ids = self.model.generate(**model_inputs, **generation_config)
                
                # è§£ç ï¼ˆåªè§£ç æ–°ç”Ÿæˆçš„ tokensï¼‰
                input_ids_len = model_inputs["input_ids"].shape[1]
                generated_text = self.processor.tokenizer.decode(
                    generated_ids[0, input_ids_len:],
                    skip_special_tokens=True
                )
                
                return generated_text.strip()
                
        except Exception as e:
            print(f"âŒ Inference failed: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def _unload_model(self):
        """å¸è½½æ¨¡å‹"""
        if self.processor is not None:
            del self.processor
            self.processor = None
        
        if self.model is not None:
            del self.model
            self.model = None
        
        self.current_model_id = None
        self.current_config = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
        
        print("ğŸ—‘ï¸  Model unloaded")
    
    def _check_disk_space(self, path: str, required_gb: float = 15):
        """æ£€æŸ¥ç£ç›˜ç©ºé—´"""
        try:
            os.makedirs(os.path.dirname(path), exist_ok=True)
            total, used, free = shutil.disk_usage(os.path.dirname(path))
            free_gb = free / (1024**3)
            
            if free_gb < required_gb:
                raise RuntimeError(
                    f"Insufficient disk space. Required: {required_gb}GB, "
                    f"Available: {free_gb:.1f}GB"
                )
        except Exception as e:
            print(f"âš ï¸  Could not check disk space: {e}")
    
    def unload(self):
        """å…¬å¼€çš„å¸è½½æ–¹æ³•"""
        self._unload_model()
