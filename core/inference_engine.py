"""
Inference Engine - è´Ÿè´£æ¨¡å‹æ¨ç†å’Œç”Ÿæˆ
"""

from typing import Dict, List, Optional, Any
import numpy as np


class InferenceEngine:
    """GGUF æ¨¡å‹æ¨ç†å¼•æ“"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨ç†å¼•æ“"""
        self.loaded_models: Dict[str, Any] = {}
        self.model_contexts: Dict[str, Any] = {}
    
    def load_model(self, model_path: str, **kwargs) -> bool:
        """
        åŠ è½½æ¨¡å‹åˆ°å†…å­˜
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            **kwargs: é¢å¤–çš„åŠ è½½å‚æ•°
        
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            from llama_cpp import Llama
            from llama_cpp.llama_chat_format import Llava15ChatHandler
            
            # æ£€æŸ¥æ˜¯å¦å·²åŠ è½½
            if model_path in self.loaded_models:
                print(f"â„¹ï¸ Model already loaded: {model_path}")
                return True
            
            # éªŒè¯æ¨¡å‹æ–‡ä»¶å­˜åœ¨
            import os
            if not os.path.exists(model_path):
                print(f"âŒ Model file not found: {model_path}")
                return False
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(model_path) / (1024**3)  # GB
            print(f"ğŸ“Š Model file size: {file_size:.2f} GB")
            
            # åŠ è½½æ¨¡å‹
            n_ctx = kwargs.get('n_ctx', 8192)
            n_gpu_layers = kwargs.get('n_gpu_layers', -1)
            verbose = kwargs.get('verbose', False)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è§†è§‰æ¨¡å‹
            mmproj_path = kwargs.get('mmproj_path')
            
            print(f"ğŸ”§ Loading parameters:")
            print(f"   - n_ctx: {n_ctx}")
            print(f"   - n_gpu_layers: {n_gpu_layers}")
            print(f"   - verbose: {verbose}")
            
            if mmproj_path:
                # éªŒè¯mmprojæ–‡ä»¶å­˜åœ¨
                if not os.path.exists(mmproj_path):
                    print(f"âŒ mmproj file not found: {mmproj_path}")
                    return False
                
                mmproj_size = os.path.getsize(mmproj_path) / (1024**2)  # MB
                print(f"   - mmproj: {mmproj_path} ({mmproj_size:.2f} MB)")
                
                # è§†è§‰è¯­è¨€æ¨¡å‹
                print("ğŸ”„ Loading vision model with mmproj...")
                chat_handler = Llava15ChatHandler(clip_model_path=mmproj_path, verbose=verbose)
                llm = Llama(
                    model_path=model_path,
                    chat_handler=chat_handler,
                    n_ctx=n_ctx,
                    n_gpu_layers=n_gpu_layers,
                    verbose=verbose,
                    logits_all=True
                )
            else:
                # çº¯æ–‡æœ¬æ¨¡å‹
                print("ğŸ”„ Loading text model...")
                llm = Llama(
                    model_path=model_path,
                    n_ctx=n_ctx,
                    n_gpu_layers=n_gpu_layers,
                    verbose=verbose
                )
            
            self.loaded_models[model_path] = llm
            print(f"âœ… Model loaded successfully: {os.path.basename(model_path)}")
            return True
            
        except FileNotFoundError as e:
            print(f"âŒ File not found error: {e}")
            print(f"   Model path: {model_path}")
            if mmproj_path:
                print(f"   mmproj path: {mmproj_path}")
            return False
        except ImportError as e:
            print(f"âŒ Import error: {e}")
            print("   Make sure llama-cpp-python is installed correctly")
            return False
        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            print(f"   Model path: {model_path}")
            import traceback
            print(f"   Traceback:\n{traceback.format_exc()}")
            return False
    
    def unload_model(self, model_path: str):
        """
        å¸è½½æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        """
        if model_path in self.loaded_models:
            del self.loaded_models[model_path]
            if model_path in self.model_contexts:
                del self.model_contexts[model_path]
    
    def generate_text(
        self,
        model_path: str,
        prompt: str,
        max_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        **kwargs
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            prompt: è¾“å…¥æç¤º
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top-p é‡‡æ ·å‚æ•°
            **kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if model_path not in self.loaded_models:
            raise ValueError(f"Model not loaded: {model_path}")
        
        llm = self.loaded_models[model_path]
        
        try:
            output = llm(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                echo=False,
                **kwargs
            )
            
            return output['choices'][0]['text']
        
        except Exception as e:
            print(f"âŒ Generation failed: {e}")
            return f"Error: {str(e)}"
    
    def generate_with_image(
        self,
        model_path: str,
        image_data: Any,
        prompt: str,
        max_tokens: int = 512,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        """
        ä½¿ç”¨å›¾åƒç”Ÿæˆæ–‡æœ¬ï¼ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            image_data: å›¾åƒæ•°æ®
            prompt: æ–‡æœ¬æç¤º
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            temperature: æ¸©åº¦å‚æ•°
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if model_path not in self.loaded_models:
            raise ValueError(f"Model not loaded: {model_path}")
        
        llm = self.loaded_models[model_path]
        
        try:
            # æ„å»ºæ¶ˆæ¯æ ¼å¼
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image_url", "image_url": {"url": image_data}},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            output = llm.create_chat_completion(
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                **kwargs
            )
            
            return output['choices'][0]['message']['content']
        
        except Exception as e:
            print(f"âŒ Vision generation failed: {e}")
            return f"Error: {str(e)}"
    
    def is_model_loaded(self, model_path: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
        return model_path in self.loaded_models
    
    def get_loaded_models(self) -> List[str]:
        """è·å–æ‰€æœ‰å·²åŠ è½½çš„æ¨¡å‹è·¯å¾„"""
        return list(self.loaded_models.keys())
    
    def clear_all(self):
        """æ¸…é™¤æ‰€æœ‰å·²åŠ è½½çš„æ¨¡å‹"""
        # æ˜¾å¼åˆ é™¤æ¨¡å‹å¯¹è±¡ä»¥é‡Šæ”¾å†…å­˜
        for model_path in list(self.loaded_models.keys()):
            try:
                del self.loaded_models[model_path]
            except:
                pass
        
        self.loaded_models.clear()
        self.model_contexts.clear()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        
        # å¦‚æœä½¿ç”¨CUDAï¼Œæ¸…ç†GPUç¼“å­˜
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                print("âœ… GPU cache cleared")
        except ImportError:
            pass
