"""
Nexa SDK Text Node - ä½¿ç”¨ Nexa SDK æœåŠ¡çš„æ–‡æœ¬ç”ŸæˆèŠ‚ç‚¹
æ”¯æŒæœ¬åœ°æ¨¡å‹è·¯å¾„ç®¡ç†ã€è‡ªåŠ¨ä¸‹è½½å’Œä¸ ComfyUI çš„ /models/LLM ç›®å½•é›†æˆ
"""

import re
import os
from typing import Tuple
from comfy.comfy_types import IO

# å°è¯•å¯¼å…¥è·¯å¾„é…ç½®
try:
    from ..config.paths import PathConfig
    HAS_PATH_CONFIG = True
except:
    HAS_PATH_CONFIG = False
    print("âš ï¸  PathConfig not available, using default paths")

from ..core.inference.nexa_engine import get_nexa_engine


# Nexa SDK é¢„è®¾æ¨¡å‹åˆ—è¡¨
# æ ¼å¼: author/model-name:quant
# ä½¿ç”¨å‰éœ€è¦å…ˆè¿è¡Œ: nexa pull <model-name>
PRESET_MODELS = [
    "Custom (è¾“å…¥è‡ªå®šä¹‰æ¨¡å‹ ID)",
    "DavidAU/Qwen3-8B-64k-Josiefied-Uncensored-HORROR-Max-GGUF:Q6_K",
    "mradermacher/Huihui-Qwen3-4B-Thinking-2507-abliterated-GGUF:Q8_0",
    "prithivMLmods/Qwen3-4B-2507-abliterated-GGUF:Q8_0",
    "mradermacher/Qwen3-4B-Thinking-2507-Uncensored-Fixed-GGUF:Q8_0",
    "mradermacher/Qwen3-Short-Story-Instruct-Uncensored-262K-ctx-4B-GGUF:Q8_0",
]

# HuggingFace URL åˆ°æ¨¡å‹ ID çš„æ˜ å°„
HUGGINGFACE_URL_MAPPING = {
    "https://huggingface.co/prithivMLmods/Qwen3-4B-2507-abliterated-GGUF/blob/main/Qwen3-4B-Instruct-2507-abliterated-GGUF/Qwen3-4B-Instruct-2507-abliterated.Q8_0.gguf": "ğŸ¤– prithivMLmods/Qwen3-4B-2507-abliterated-GGUF:Q8_0",
    
    "https://huggingface.co/mradermacher/Qwen3-4B-Thinking-2507-Uncensored-Fixed-GGUF/resolve/main/Qwen3-4B-Thinking-2507-Uncensored-Fixed.Q8_0.gguf": "ğŸ¤– mradermacher/Qwen3-4B-Thinking-2507-Uncensored-Fixed-GGUF:Q8_0",
    
    "https://huggingface.co/mradermacher/Qwen3-Short-Story-Instruct-Uncensored-262K-ctx-4B-GGUF/blob/main/Qwen3-Short-Story-Instruct-Uncensored-262K-ctx-4B.Q8_0.gguf": "ğŸ¤– mradermacher/Qwen3-Short-Story-Instruct-Uncensored-262K-ctx-4B-GGUF:Q8_0",
    
    "https://huggingface.co/Triangle104/Josiefied-Qwen3-4B-abliterated-v2-Q8_0-GGUF/blob/main/josiefied-qwen3-4b-abliterated-v2-q8_0.gguf": "ğŸ¤– Triangle104/Josiefied-Qwen3-4B-abliterated-v2-Q8_0-GGUF",
}


def parse_model_input(model_input: str) -> str:
    """
    è§£ææ¨¡å‹è¾“å…¥ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼š
    1. æ¨¡å‹ ID: "user/repo:quantization"
    2. HuggingFace URL
    3. æœ¬åœ°æ–‡ä»¶å: "model.gguf"
    
    Returns:
        æ ‡å‡†åŒ–çš„æ¨¡å‹æ ‡è¯†ç¬¦
    """
    model_input = model_input.strip()
    
    # å¦‚æœæ˜¯ HuggingFace URLï¼Œè½¬æ¢ä¸ºæ¨¡å‹ ID
    if model_input.startswith("https://huggingface.co/"):
        if model_input in HUGGINGFACE_URL_MAPPING:
            return HUGGINGFACE_URL_MAPPING[model_input]
        
        # å°è¯•ä» URL ä¸­æå–æ¨¡å‹ä¿¡æ¯
        # æ ¼å¼: https://huggingface.co/user/repo/blob/main/file.gguf
        # æˆ–: https://huggingface.co/user/repo/resolve/main/file.gguf
        parts = model_input.replace("https://huggingface.co/", "").split("/")
        if len(parts) >= 2:
            user = parts[0]
            repo = parts[1]
            
            # æå–é‡åŒ–ç±»å‹ï¼ˆå¦‚æœæœ‰ï¼‰
            if len(parts) >= 4:
                filename = parts[-1]
                # ä»æ–‡ä»¶åæå–é‡åŒ–ç±»å‹ï¼Œå¦‚ Q8_0, Q6_K ç­‰
                import re
                quant_match = re.search(r'\.(Q\d+_[0K]|Q\d+)', filename, re.IGNORECASE)
                if quant_match:
                    quant = quant_match.group(1).upper()
                    return f"{user}/{repo}:{quant}"
            
            return f"{user}/{repo}"
    
    # ç›´æ¥è¿”å›ï¼ˆæ¨¡å‹ ID æˆ–æœ¬åœ°æ–‡ä»¶åï¼‰
    return model_input


class RemoteAPIConfig:
    """è¿œç¨‹ API é…ç½®èŠ‚ç‚¹ï¼ˆNexa/Ollamaï¼‰"""
    
    @staticmethod
    def get_available_models(base_url="http://127.0.0.1:11434", api_type="ollama"):
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            # å¦‚æœ base_url ä¸­æŒ‡å®šäº†ç«¯å£ï¼Œä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„
            if ':' in base_url.split('//')[-1]:
                try:
                    engine = get_nexa_engine(base_url)
                    if engine.is_service_available():
                        models = engine.get_available_models(force_refresh=False)
                        if models:
                            return models
                except:
                    pass
            
            # æ ¹æ® API ç±»å‹é€‰æ‹©ç«¯å£æ‰«æé¡ºåº
            api_type_lower = api_type.lower()
            if api_type_lower == "ollama":
                ports_to_try = [11434]  # Ollama å®˜æ–¹é»˜è®¤ç«¯å£
            elif api_type_lower in ["lm studio", "lmstudio"]:
                ports_to_try = [1234]  # LM Studio å®˜æ–¹é»˜è®¤ç«¯å£
            else:  # Nexa SDK
                ports_to_try = [8080]  # Nexa SDK å®˜æ–¹é»˜è®¤ç«¯å£
            
            # å°è¯•å¸¸ç”¨ç«¯å£
            for port in ports_to_try:
                try:
                    test_url = f"http://127.0.0.1:{port}"
                    engine = get_nexa_engine(test_url)
                    if engine.is_service_available():
                        models = engine.get_available_models(force_refresh=False)
                        if models:
                            return models
                except:
                    continue
            
            return ["(è¯·ç‚¹å‡»åˆ·æ–°æŒ‰é’®)"]
        except:
            return ["(è¯·ç‚¹å‡»åˆ·æ–°æŒ‰é’®)"]
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "base_url": ("STRING", {
                    "default": "http://127.0.0.1:11434",
                    "multiline": False,
                    "tooltip": "API æœåŠ¡åœ°å€ï¼ˆOllama: 11434, Nexa: 8080, LM Studio: 1234ï¼‰"
                }),
                "api_type": (["Ollama", "Nexa SDK", "LM Studio"], {
                    "default": "Ollama",
                    "tooltip": "API ç±»å‹ï¼ˆLM Studio ä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼ï¼‰"
                }),
                # ä½¿ç”¨ç©ºå…ƒç»„è¡¨ç¤ºåŠ¨æ€åˆ—è¡¨ï¼Œç”±å‰ç«¯ JavaScript æ§åˆ¶
                "model": ((), {
                    "tooltip": "é€‰æ‹©æ¨¡å‹ï¼ˆç‚¹å‡» ğŸ”„ Refresh Models æŒ‰é’®æ›´æ–°åˆ—è¡¨ï¼‰"
                }),
            },
            "optional": {
                "system_prompt": (IO.STRING, {
                    "default": "",
                    "multiline": True,
                    "tooltip": "ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰"
                }),
            }
        }
    
    RETURN_TYPES = ("TEXT_MODEL", "STRING")
    RETURN_NAMES = ("model_config", "status_info")
    FUNCTION = "configure_api"
    CATEGORY = "ğŸ¤– GGUF-VLM/ğŸ’¬ Text Models"
    OUTPUT_NODE = True
    
    def configure_api(
        self, 
        base_url: str,
        api_type: str,
        model: str,
        system_prompt: str = ""
    ):
        """é…ç½®è¿œç¨‹ API"""
        
        print(f"\n{'='*80}")
        print(f" ğŸŒ Remote API Config (Nexa/Ollama/LM Studio)")
        print(f"{'='*80}")
        
        # æ˜ å°„ API ç±»å‹
        api_type_map = {
            "Nexa SDK": "nexa",
            "Ollama": "ollama",
            "LM Studio": "lmstudio"
        }
        api_key = api_type_map.get(api_type, "ollama")
        
        # åˆ›å»ºæˆ–è·å–å¼•æ“
        engine = get_nexa_engine(base_url)
        
        # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
        print(f"ğŸ” æ£€æµ‹æœåŠ¡è¿é€šæ€§...")
        print(f"   URL: {base_url}")
        print(f"   Type: {api_type}")
        
        is_available = engine.is_service_available()
        
        if not is_available:
            error_msg = f"âŒ {api_type} æœåŠ¡ä¸å¯ç”¨"
            status_info = f"âŒ è¿æ¥å¤±è´¥\n"
            status_info += f"URL: {base_url}\n"
            status_info += f"è¯·ç¡®ä¿æœåŠ¡æ­£åœ¨è¿è¡Œ\n"
            status_info += f"\næç¤ºï¼š\n"
            status_info += f"- Ollama: è¿è¡Œ 'ollama serve'\n"
            status_info += f"- Nexa SDK: è¿è¡Œ 'nexa serve'\n"
            status_info += f"- LM Studio: å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨ (ç«¯å£ 1234)"
            
            print(error_msg)
            print(f"   URL: {base_url}")
            print(f"   è¯·ç¡®ä¿æœåŠ¡æ­£åœ¨è¿è¡Œ")
            print(f"{'='*80}\n")
            
            config = {
                "mode": "remote",
                "base_url": base_url,
                "api_type": api_key,
                "model_name": "",
                "system_prompt": system_prompt,
                "service_available": False,
                "available_models": [],
                "error": error_msg
            }
            return (config, status_info)
        
        # è·å–å¯ç”¨æ¨¡å‹
        print(f"ğŸ“‹ è·å–æ¨¡å‹åˆ—è¡¨...")
        available_models = engine.get_available_models(force_refresh=False)
        
        # ç¡®å®šä½¿ç”¨çš„æ¨¡å‹
        if model and model.strip() and not model.startswith("(") and not model.startswith("âŒ") and not model.startswith("âš ï¸"):
            # ç”¨æˆ·é€‰æ‹©äº†æœ‰æ•ˆçš„æ¨¡å‹
            selected_model = model.strip()
            print(f"   ä½¿ç”¨é€‰æ‹©çš„æ¨¡å‹: {selected_model}")
        elif available_models and available_models[0] and not available_models[0].startswith("("):
            # è‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
            selected_model = available_models[0]
            print(f"   è‡ªåŠ¨é€‰æ‹©æ¨¡å‹: {selected_model}")
        else:
            selected_model = ""
            print(f"   âš ï¸  æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹ï¼Œè¯·ç‚¹å‡» ğŸ”„ Refresh Models æŒ‰é’®")
        
        if not available_models or not selected_model:
            status_info = f"âš ï¸ æœåŠ¡å·²è¿æ¥ï¼Œä½†æœªæ‰¾åˆ°æ¨¡å‹\n"
            status_info += f"URL: {base_url}\n"
            status_info += f"å¯ç”¨æ¨¡å‹: {len(available_models) if available_models else 0}\n"
            status_info += f"\nè¯·ç‚¹å‡» ğŸ”„ Refresh Models æŒ‰é’®åˆ·æ–°æ¨¡å‹åˆ—è¡¨"
            
            config = {
                "mode": "remote",
                "base_url": base_url,
                "api_type": api_key,
                "model_name": selected_model,
                "system_prompt": system_prompt,
                "service_available": True,
                "available_models": available_models or []
            }
            return (config, status_info)
        
        # æ„å»ºçŠ¶æ€ä¿¡æ¯
        status_info = f"âœ… è¿æ¥æˆåŠŸ\n"
        status_info += f"URL: {base_url}\n"
        status_info += f"ç±»å‹: {api_type}\n"
        status_info += f"å¯ç”¨æ¨¡å‹: {len(available_models)}\n"
        status_info += f"é»˜è®¤æ¨¡å‹: {selected_model}\n"
        status_info += f"\næ¨¡å‹åˆ—è¡¨:\n"
        for i, model in enumerate(available_models[:10], 1):
            status_info += f"  {i}. {model}\n"
        if len(available_models) > 10:
            status_info += f"  ... è¿˜æœ‰ {len(available_models) - 10} ä¸ªæ¨¡å‹"
        
        # åˆ›å»ºé…ç½®ï¼ˆä½¿ç”¨ TEXT_MODEL æ ¼å¼ï¼Œå…¼å®¹ TextGeneration èŠ‚ç‚¹ï¼‰
        config = {
            "mode": "remote",
            "base_url": base_url,
            "api_type": api_key,
            "model_name": selected_model,
            "system_prompt": system_prompt,
            "service_available": True,
            "available_models": available_models
        }
        
        print(f"âœ… {api_type} é…ç½®æˆåŠŸ")
        print(f"   URL: {base_url}")
        print(f"   é»˜è®¤æ¨¡å‹: {selected_model}")
        print(f"   å¯ç”¨æ¨¡å‹æ•°: {len(available_models)}")
        if available_models:
            print(f"   æ¨¡å‹åˆ—è¡¨:")
            for i, model in enumerate(available_models[:5], 1):
                print(f"      {i}. {model}")
            if len(available_models) > 5:
                print(f"      ... è¿˜æœ‰ {len(available_models) - 5} ä¸ªæ¨¡å‹")
        print(f"{'='*80}\n")
        
        return (config, status_info)


# RemoteTextGeneration èŠ‚ç‚¹å·²ç§»é™¤
# è¯·ä½¿ç”¨ unified_text_node.py ä¸­çš„ TextGeneration èŠ‚ç‚¹
# RemoteAPIConfig ç°åœ¨è¾“å‡º TEXT_MODEL ç±»å‹ï¼Œå¯ä»¥ç›´æ¥è¿æ¥åˆ° TextGeneration


class NexaServiceStatus:
    """Nexa SDK æœåŠ¡çŠ¶æ€æ£€æŸ¥èŠ‚ç‚¹"""
    
    @classmethod
    def INPUT_TYPES(cls):
        # è·å– LLM æ¨¡å‹ç›®å½•
        if HAS_PATH_CONFIG:
            default_models_dir = PathConfig.get_llm_models_path()
        else:
            import folder_paths
            default_models_dir = os.path.join(folder_paths.models_dir, "LLM", "GGUF")
            os.makedirs(default_models_dir, exist_ok=True)
        
        return {
            "required": {
                "base_url": ("STRING", {
                    "default": "http://127.0.0.1:8080",
                    "tooltip": "Nexa SDK æœåŠ¡åœ°å€ï¼ˆé»˜è®¤: 8080ï¼‰"
                }),
                "models_dir": ("STRING", {
                    "default": default_models_dir,
                    "tooltip": "æœ¬åœ°æ¨¡å‹ç›®å½•"
                }),
                "refresh": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "åˆ·æ–°æ¨¡å‹åˆ—è¡¨"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("status", "remote_models", "local_models")
    FUNCTION = "check_status"
    CATEGORY = "ğŸ¤– GGUF-VLM/ğŸ› ï¸ Tools"
    OUTPUT_NODE = True
    
    def check_status(self, base_url: str, models_dir: str, refresh: bool = False):
        """æ£€æŸ¥æœåŠ¡çŠ¶æ€"""
        
        engine = get_nexa_engine(base_url, models_dir)
        
        # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
        is_available = engine.is_service_available()
        
        status_lines = []
        status_lines.append(f"Nexa SDK Service: {base_url}")
        status_lines.append(f"Models Directory: {models_dir}")
        status_lines.append("")
        
        if is_available:
            # è·å–è¿œç¨‹æ¨¡å‹åˆ—è¡¨
            remote_models = engine.get_available_models(force_refresh=refresh)
            
            status_lines.append(f"âœ… Service is AVAILABLE")
            status_lines.append(f"Found {len(remote_models)} remote model(s)")
            
            remote_models_str = "\n".join([f"  - {model}" for model in remote_models]) if remote_models else "  (none)"
        else:
            status_lines.append(f"âŒ Service is NOT AVAILABLE")
            status_lines.append("Please make sure the service is running.")
            remote_models_str = "Service unavailable"
        
        # è·å–æœ¬åœ°æ¨¡å‹åˆ—è¡¨
        local_models = engine.get_local_models()
        status_lines.append(f"Found {len(local_models)} local model(s)")
        
        local_models_str = "\n".join([f"  - {model}" for model in local_models]) if local_models else "  (none)"
        
        status = "\n".join(status_lines)
        
        print(status)
        print("\nRemote models:")
        print(remote_models_str)
        print("\nLocal models:")
        print(local_models_str)
        
        return (status, remote_models_str, local_models_str)


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "RemoteAPIConfig": RemoteAPIConfig,
    "NexaServiceStatus": NexaServiceStatus,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "RemoteAPIConfig": "ğŸŒ Remote API Config (Ollama/Nexa/LM Studio)",
    "NexaServiceStatus": "ğŸ“Š Service Status Check",
}
