"""
Memory Manager Node - æ˜¾å­˜/å†…å­˜ç®¡ç†èŠ‚ç‚¹
ç”¨äºæ‰‹åŠ¨é‡Šæ”¾æ¨¡å‹å ç”¨çš„æ˜¾å­˜å’Œå†…å­˜
"""

import gc
import sys
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
module_path = Path(__file__).parent.parent
if str(module_path) not in sys.path:
    sys.path.insert(0, str(module_path))

try:
    from core.inference_engine import InferenceEngine
except ImportError as e:
    print(f"[ComfyUI-GGUF-VLM] Import error in memory_manager_node: {e}")
    from ..core.inference_engine import InferenceEngine


class MemoryManagerNode:
    """æ˜¾å­˜/å†…å­˜ç®¡ç†èŠ‚ç‚¹"""
    
    # å…¨å±€æ¨ç†å¼•æ“å¼•ç”¨
    _inference_engine = None
    
    @classmethod
    def _get_engine(cls):
        """è·å–æ¨ç†å¼•æ“"""
        if cls._inference_engine is None:
            cls._inference_engine = InferenceEngine()
        return cls._inference_engine
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "action": (["Clear All Models", "Force GC", "Clear GPU Cache", "Full Cleanup"], {
                    "default": "Full Cleanup",
                    "tooltip": "é€‰æ‹©æ¸…ç†æ“ä½œ"
                }),
            },
            "optional": {
                "trigger": ("*", {
                    "tooltip": "è¿æ¥ä»»æ„è¾“å‡ºä»¥è§¦å‘æ¸…ç†ï¼ˆå¯é€‰ï¼‰"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("status",)
    FUNCTION = "manage_memory"
    CATEGORY = "ğŸ¤– GGUF-VLM/âš™ï¸ Utils"
    OUTPUT_NODE = True
    
    def manage_memory(self, action, trigger=None):
        """æ‰§è¡Œå†…å­˜ç®¡ç†æ“ä½œ"""
        try:
            status_messages = []
            
            if action == "Clear All Models" or action == "Full Cleanup":
                # æ¸…é™¤æ‰€æœ‰å·²åŠ è½½çš„æ¨¡å‹
                engine = self._get_engine()
                loaded_models = engine.get_loaded_models()
                
                if loaded_models:
                    status_messages.append(f"ğŸ—‘ï¸ Unloading {len(loaded_models)} model(s)...")
                    for model_path in loaded_models:
                        status_messages.append(f"   - {model_path}")
                    
                    engine.clear_all()
                    status_messages.append("âœ… All models unloaded")
                else:
                    status_messages.append("â„¹ï¸ No models currently loaded")
            
            if action == "Force GC" or action == "Full Cleanup":
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                collected = gc.collect()
                status_messages.append(f"ğŸ§¹ Garbage collection: {collected} objects collected")
            
            if action == "Clear GPU Cache" or action == "Full Cleanup":
                # æ¸…ç†GPUç¼“å­˜
                try:
                    import torch
                    if torch.cuda.is_available():
                        # è·å–æ¸…ç†å‰çš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
                        before_allocated = torch.cuda.memory_allocated() / 1024**3
                        before_reserved = torch.cuda.memory_reserved() / 1024**3
                        
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                        
                        # è·å–æ¸…ç†åçš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
                        after_allocated = torch.cuda.memory_allocated() / 1024**3
                        after_reserved = torch.cuda.memory_reserved() / 1024**3
                        
                        freed_allocated = before_allocated - after_allocated
                        freed_reserved = before_reserved - after_reserved
                        
                        status_messages.append("ğŸ® GPU cache cleared")
                        status_messages.append(f"   Allocated: {before_allocated:.2f}GB â†’ {after_allocated:.2f}GB (freed: {freed_allocated:.2f}GB)")
                        status_messages.append(f"   Reserved: {before_reserved:.2f}GB â†’ {after_reserved:.2f}GB (freed: {freed_reserved:.2f}GB)")
                    else:
                        status_messages.append("â„¹ï¸ CUDA not available, skipping GPU cache clear")
                except ImportError:
                    status_messages.append("âš ï¸ PyTorch not available, cannot clear GPU cache")
                except Exception as e:
                    status_messages.append(f"âš ï¸ Error clearing GPU cache: {e}")
            
            # ç»„åˆæ‰€æœ‰çŠ¶æ€æ¶ˆæ¯
            status = "\n".join(status_messages)
            print(f"\n{'='*60}")
            print("ğŸ§¹ Memory Manager")
            print(f"{'='*60}")
            print(status)
            print(f"{'='*60}\n")
            
            return (status,)
        
        except Exception as e:
            import traceback
            error_msg = f"âŒ Memory management error: {str(e)}\n{traceback.format_exc()}"
            print(error_msg)
            return (error_msg,)


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "MemoryManagerNode": MemoryManagerNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "MemoryManagerNode": "ğŸ§¹ Memory Manager (GGUF)",
}
