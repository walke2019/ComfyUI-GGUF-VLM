"""
Vision Node (Transformers Mode) - Transformers æ¨¡å¼çš„è§†è§‰è¯­è¨€æ¨¡å‹èŠ‚ç‚¹
æ”¯æŒ Qwen3-VL ç­‰å®Œæ•´çš„ Transformers æ¨¡å‹ï¼ˆä½¿ç”¨æœ€æ–° APIï¼‰
"""

import os
import sys
import torch
from pathlib import Path
from PIL import Image
from torchvision.transforms import ToPILImage
import folder_paths
from comfy.comfy_types import IO

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
module_path = Path(__file__).parent.parent
if str(module_path) not in sys.path:
    sys.path.insert(0, str(module_path))

try:
    from core.inference.transformers_engine import TransformersInferenceEngine
    from utils.system_prompts import SystemPromptsManager
    from config.node_definitions import (
        SEED_INPUT,
        TEMPERATURE_INPUT,
        TOP_P_INPUT,
        TOP_K_INPUT,
        REPETITION_PENALTY_INPUT,
        PROMPT_INPUT,
        SYSTEM_PROMPT_INPUT,
        TRANSFORMERS_QUANTIZATION_INPUT,
        TRANSFORMERS_ATTENTION_INPUT,
        TRANSFORMERS_PIXELS_INPUT,
        KEEP_MODEL_LOADED_INPUT,
        TEXT_OUTPUT,
        TRANSFORMERS_MODEL_OUTPUT,
        merge_inputs
    )
except ImportError:
    from ..core.inference.transformers_engine import TransformersInferenceEngine
    from ..utils.system_prompts import SystemPromptsManager
    from ..config.node_definitions import (
        SEED_INPUT,
        TEMPERATURE_INPUT,
        TOP_P_INPUT,
        TOP_K_INPUT,
        REPETITION_PENALTY_INPUT,
        PROMPT_INPUT,
        SYSTEM_PROMPT_INPUT,
        TRANSFORMERS_QUANTIZATION_INPUT,
        TRANSFORMERS_ATTENTION_INPUT,
        TRANSFORMERS_PIXELS_INPUT,
        KEEP_MODEL_LOADED_INPUT,
        TEXT_OUTPUT,
        TRANSFORMERS_MODEL_OUTPUT,
        merge_inputs
    )


class VisionModelLoaderTransformers:
    """Transformers æ¨¡å¼çš„è§†è§‰æ¨¡å‹åŠ è½½å™¨"""
    
    # å…¨å±€å¼•æ“å®ä¾‹
    _engine = None
    
    @classmethod
    def _get_engine(cls):
        """è·å–å…¨å±€å¼•æ“å®ä¾‹"""
        if cls._engine is None:
            cls._engine = TransformersInferenceEngine()
        return cls._engine
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": merge_inputs(
                {
                    "model": (
                        [
                            "Huihui-Qwen3-VL-4B-Instruct-abliterated",
                            "Huihui-Qwen3-VL-8B-Instruct-abliterated"
                        ],
                        {
                            "default": "Huihui-Qwen3-VL-4B-Instruct-abliterated",
                            "tooltip": "é€‰æ‹© Qwen3-VL Abliterated æ¨¡å‹"
                        }
                    ),
                },
                TRANSFORMERS_QUANTIZATION_INPUT,
                TRANSFORMERS_ATTENTION_INPUT,
                KEEP_MODEL_LOADED_INPUT,
                TRANSFORMERS_PIXELS_INPUT
            )
        }
    
    RETURN_TYPES = TRANSFORMERS_MODEL_OUTPUT["types"]
    RETURN_NAMES = TRANSFORMERS_MODEL_OUTPUT["names"]
    FUNCTION = "load_model"
    CATEGORY = "ğŸ¤– GGUF-VLM/ğŸ–¼ï¸ Vision Models"
    
    def load_model(
        self,
        model,
        quantization,
        attention,
        keep_model_loaded,
        min_pixels,
        max_pixels
    ):
        """åŠ è½½ Transformers æ¨¡å‹"""
        
        # ç¡®å®šæ¨¡å‹ ID
        if model == "Huihui-Qwen3-VL-8B-Instruct-abliterated":
            model_id = "huihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated"
        elif model == "Huihui-Qwen3-VL-4B-Instruct-abliterated":
            model_id = "huihui-ai/Huihui-Qwen3-VL-4B-Instruct-abliterated"
        else:
            model_id = f"qwen/{model}"
        
        # æ„å»ºé…ç½®
        config = {
            "model_name": model,
            "model_id": model_id,
            "quantization": quantization,
            "attention": attention,
            "min_pixels": min_pixels,
            "max_pixels": max_pixels,
            "keep_loaded": keep_model_loaded,
        }
        
        # åŠ è½½æ¨¡å‹
        engine = self._get_engine()
        success = engine.load_model(config)
        
        if not success:
            raise RuntimeError(f"Failed to load model: {model}")
        
        print(f"âœ… Transformers model loaded: {model}")
        
        return (config,)


class VisionLanguageNodeTransformers:
    """Transformers æ¨¡å¼çš„è§†è§‰è¯­è¨€èŠ‚ç‚¹ï¼ˆQwen3-VL ä¼˜åŒ–ï¼‰"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": merge_inputs(
                {
                    "model_config": ("TRANSFORMERS_MODEL",),
                    "prompt": (IO.STRING, {"default": "Describe this image.", "multiline": False, "tooltip": "ç”¨æˆ·æç¤ºè¯"}),
                    "max_tokens": (
                        "INT",
                        {
                            "default": 512,
                            "min": 128,
                            "max": 256000,
                            "step": 1,
                            "tooltip": "æœ€å¤§ç”Ÿæˆ token æ•°"
                        }
                    ),
                },
                TEMPERATURE_INPUT,
                TOP_P_INPUT,
                TOP_K_INPUT,
                REPETITION_PENALTY_INPUT,
                SEED_INPUT
            ),
            "optional": merge_inputs(
                {
                    "image": ("IMAGE",),
                },
                SYSTEM_PROMPT_INPUT
            )
        }
    
    RETURN_TYPES = TEXT_OUTPUT["types"]
    RETURN_NAMES = TEXT_OUTPUT["names"]
    FUNCTION = "generate"
    CATEGORY = "ğŸ¤– GGUF-VLM/ğŸ–¼ï¸ Vision Models"
    OUTPUT_NODE = True
    
    def generate(
        self,
        model_config,
        prompt,
        temperature,
        top_p,
        top_k,
        repetition_penalty,
        max_tokens,
        seed,
        image=None,
        system_prompt=""
    ):
        """ç”Ÿæˆæ–‡æœ¬ï¼ˆä½¿ç”¨ Qwen3-VL æ–° APIï¼‰"""
        
        engine = VisionModelLoaderTransformers._get_engine()
        
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        if engine.model is None or engine.processor is None:
            print("âš ï¸  Model not loaded, loading now...")
            success = engine.load_model(model_config)
            if not success:
                raise RuntimeError(f"Failed to load model: {model_config.get('model_name', 'unknown')}")
        
        # å‡†å¤‡å›¾åƒæˆ–è§†é¢‘ï¼ˆæ”¯æŒå¤šå¸§ï¼‰
        temp_paths = []
        if image is not None:
            # æ£€æŸ¥æ˜¯å•å¸§å›¾åƒè¿˜æ˜¯è§†é¢‘å¸§åºåˆ—
            num_frames = image.shape[0]
            
            if num_frames == 1:
                # å•å¸§å›¾åƒ
                pil_image = ToPILImage()(image[0].permute(2, 0, 1))
                temp_path = Path(folder_paths.temp_directory) / f"temp_image_{seed}.png"
                pil_image.save(temp_path)
                temp_paths.append(temp_path)
                print(f"ğŸ“¸ Processing single image")
            else:
                # è§†é¢‘å¸§åºåˆ—ï¼šä¿å­˜æ‰€æœ‰å¸§
                print(f"ğŸ“¹ Processing video with {num_frames} frames")
                for frame_idx in range(num_frames):
                    pil_image = ToPILImage()(image[frame_idx].permute(2, 0, 1))
                    temp_path = Path(folder_paths.temp_directory) / f"temp_video_{seed}_frame_{frame_idx:04d}.png"
                    pil_image.save(temp_path)
                    temp_paths.append(temp_path)
        
        # æ„å»ºæ¶ˆæ¯ï¼ˆQwen3-VL æ ¼å¼ - æ”¯æŒå¤šå¸§è§†é¢‘åˆ†æï¼‰
        messages = []
        
        # æ„å»ºç”¨æˆ·æ¶ˆæ¯å†…å®¹
        user_content = []
        
        # æ·»åŠ æ‰€æœ‰å›¾åƒ/è§†é¢‘å¸§
        for temp_path in temp_paths:
            user_content.append({
                "type": "image",
                "image": str(temp_path)
            })
        
        # å¤„ç†æç¤ºè¯ï¼šå¦‚æœæœ‰ç³»ç»Ÿæç¤ºè¯ï¼Œå°†å…¶ä½œä¸ºæŒ‡ä»¤å‰ç¼€æ·»åŠ åˆ°ç”¨æˆ·æç¤ºè¯ä¸­
        # æ³¨æ„ï¼šä¸ä½¿ç”¨ç‹¬ç«‹çš„ system roleï¼Œè€Œæ˜¯åˆå¹¶åˆ° user æ¶ˆæ¯ä¸­
        final_prompt = prompt
        if system_prompt and system_prompt.strip():
            # ç³»ç»Ÿæç¤ºè¯ä½œä¸ºæŒ‡ä»¤å‰ç¼€
            final_prompt = f"{system_prompt.strip()}\n\n{prompt}"
        
        user_content.append({
            "type": "text",
            "text": final_prompt
        })
        
        messages.append({
            "role": "user",
            "content": user_content
        })
        
        # æ‰§è¡Œæ¨ç†ï¼ˆä½¿ç”¨ Qwen3-VL æ¨èå‚æ•°ï¼‰
        try:
            # æ‰“å°è°ƒè¯•ä¿¡æ¯
            print(f"ğŸ” Generation parameters:")
            print(f"   - Prompt: {prompt[:100]}...")
            print(f"   - Temperature: {temperature}")
            print(f"   - Max tokens: {max_tokens}")
            print(f"   - Top-p: {top_p}")
            print(f"   - Top-k: {top_k}")
            print(f"   - Repetition penalty: {repetition_penalty}")
            
            # é™åˆ¶ max_tokens é¿å…è¿‡é•¿è¾“å‡ºï¼ˆä½†å…è®¸æ›´é•¿çš„æè¿°ï¼‰
            safe_max_tokens = min(max_tokens, 1024)
            if max_tokens > 1024:
                print(f"âš ï¸  Max tokens reduced from {max_tokens} to {safe_max_tokens} to prevent runaway generation")
            
            result = engine.inference(
                messages=messages,
                temperature=temperature,
                max_new_tokens=safe_max_tokens,
                seed=seed,
                top_p=top_p,
                top_k=top_k,
                repetition_penalty=repetition_penalty
            )
            
            print(f"âœ… Generated text ({len(result)} chars)")
            print(f"ğŸ“ Output preview: {result[:200]}...")
            
            # æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
            for temp_path in temp_paths:
                if temp_path.exists():
                    temp_path.unlink()
            
            # å¦‚æœä¸ä¿æŒåŠ è½½ï¼Œå¸è½½æ¨¡å‹
            if not model_config.get("keep_loaded", False):
                engine.unload()
            
            return (result,)
            
        except Exception as e:
            print(f"âŒ Generation failed: {e}")
            import traceback
            traceback.print_exc()
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for temp_path in temp_paths:
                if temp_path.exists():
                    temp_path.unlink()
            
            raise


# å¯¼å‡ºèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "VisionModelLoaderTransformers": VisionModelLoaderTransformers,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VisionModelLoaderTransformers": "ğŸ–¼ï¸ Vision Model Loader (Transformers)",
}
